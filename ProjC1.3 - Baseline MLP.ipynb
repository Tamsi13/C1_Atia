{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHw2XgHzmQhX",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Creating a Baseline using MLPs\n",
    "\n",
    "This script will walk you through the creating of a baseline model for the data. You will have to make some choices of hyper-parameteres so you can either search for those values manually or using a gridsearch... No need to use HyperBand for this simple problem =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1843,
     "status": "ok",
     "timestamp": 1711034777901,
     "user": {
      "displayName": "Edgar Lobaton",
      "userId": "02856608958118334478"
     },
     "user_tz": 240
    },
    "id": "lM2Ii3T1mQha",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "outputId": "2c78b9ec-e84d-4e56-c8a6-3f95e67da126"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfncs_helper\u001b[39;00m\u001b[38;5;66;03m# as fncs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ML-Gait1/fncs_helper.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats \u001b[38;5;28;01mas\u001b[39;00m st\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import fncs_helper# as fncs\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "importlib.reload(fncs_helper)\n",
    "\n",
    "import json\n",
    "with open('Settings.json') as f:\n",
    "    S = json.load(f)\n",
    "    dataFolder = S['dataFolder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heZK31KGmQhc",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Loading the Data and Features\n",
    "\n",
    "First, we define a helper function to load the data and compute the features. Take a look at how the labels are extracted by following a similar procedure for window extraction and assigning the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Extracting the features for the training data and splitting it into training / validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 75965,
     "status": "ok",
     "timestamp": 1711034858184,
     "user": {
      "displayName": "Edgar Lobaton",
      "userId": "02856608958118334478"
     },
     "user_tz": 240
    },
    "id": "wKG6uX6tmQhc",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Specifying the data directory\n",
    "dirTrain = dataFolder + 'Train/'\n",
    "\n",
    "# Specifying some parameters for the feature extraction\n",
    "timeStep = 1\n",
    "winSz = 2\n",
    "\n",
    "# Specifying IDs for training and validation sets\n",
    "valIDs = [2,11,25]\n",
    "trainIDs = list(set(np.array(range(25))+1).difference(valIDs))\n",
    "\n",
    "# Recovering the features and labels\n",
    "xTrain, yTrain = fncs_helper.loadFeatures(dirTrain,winSz,timeStep,trainIDs)\n",
    "xVal, yVal = fncs_helper.loadFeatures(dirTrain,winSz,timeStep,valIDs)\n",
    "\n",
    "# Saving validation set\n",
    "np.savetxt('Data/val.x.csv',xVal,delimiter=',')\n",
    "np.savetxt('Data/val.y.csv',yVal,fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Training MLPs\n",
    "\n",
    "Our main objective will be to do some optimization on the architecture. Let us begin by defining some constant parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1711035040557,
     "user": {
      "displayName": "Edgar Lobaton",
      "userId": "02856608958118334478"
     },
     "user_tz": 240
    },
    "id": "JRa1XfOZmQhd",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining the input and output size of the model\n",
    "INPUTSIZE = 12\n",
    "OUTPUTSIZE = 4\n",
    "\n",
    "# Specify the device that you will use here. You can change redefine this variable if needed (e.g.,\n",
    "# when using Colab and wanting to use the GPUs).\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### 1-Layer MLP\n",
    "\n",
    "Our first MLP model is a single layer model. In this case, we don't have a choice on the number of neurons since the dimensions of the layer are pre-determined by the input and ouput.\n",
    "\n",
    "We begin by defining the neural network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining the neural network architecture\n",
    "class Net1(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net1, self).__init__()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(INPUTSIZE,OUTPUTSIZE)\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    logits = self.linear_relu_stack(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Defining and training a model. We are providing some default values for the number of epochs and the weights. You need to find a better set of values so you overcome the low balanced accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the settings in case you end up modifying them\n",
    "with open('Settings.json') as f:\n",
    "    S = json.load(f)\n",
    "    \n",
    "# Number of epochs specified in the settings file\n",
    "epochs = S['1Layer']['epochs']\n",
    "\n",
    "# The weights for cross-entropy specified i nthe setting file\n",
    "weights = S['1Layer']['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m M1 \u001b[38;5;241m=\u001b[39m fncs_helper\u001b[38;5;241m.\u001b[39mNetWrapper(model,DEVICE,epochs,weights)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Displaying a summary\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m summary(M1\u001b[38;5;241m.\u001b[39mmodel,(\u001b[38;5;241m1\u001b[39m,INPUTSIZE))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m M1\u001b[38;5;241m.\u001b[39mfit(xTrain,yTrain)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(fncs_helper)\n",
    "\n",
    "# Defining the Model\n",
    "model = Net1().to(DEVICE)\n",
    "M1 = fncs_helper.NetWrapper(model,DEVICE,epochs,weights)\n",
    "\n",
    "# Displaying a summary\n",
    "summary(M1.model,(1,INPUTSIZE))\n",
    "\n",
    "# Training the model\n",
    "M1.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Performance on training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "yTrainHat1 = M1.predict(xTrain)\n",
    "yValHat1 = M1.predict(xVal)\n",
    "\n",
    "print('RESULTS:\\n')\n",
    "fncs_helper.summaryPerf(yTrain,yTrainHat1,yVal,yValHat1)\n",
    "\n",
    "# Saving the predictions since they will be used for testing later\n",
    "np.savetxt('Data/1LayerPred.y.csv',yValHat1,fmt='%d')\n",
    "\n",
    "# Saving the model\n",
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save('Models/model_1Layer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Your task is to select the hyper-parameters specified in `Settings.json` for the 1-layer model so your performance exceeds 0.75 on balanced accuracy. Once you are done, can run the test below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "MLP1",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!python -m pytest fnc_1Layer_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### 2-Layer MLP\n",
    "\n",
    "In this case, we have a choice on the number of neurons for the hidden layer. Note that in this section, we will use the optimal number of epochs and weights found on the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining the neural network architecture\n",
    "class Net2(nn.Module):\n",
    "  def __init__(self,noNeurons):\n",
    "    super(Net2, self).__init__()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(INPUTSIZE,noNeurons),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(noNeurons,OUTPUTSIZE),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.linear_relu_stack(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Defining and training the model with an arbitrary number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the settings in case you end up modifying them\n",
    "with open('Settings.json') as f:\n",
    "    S = json.load(f)\n",
    "    \n",
    "# Number of epochs specified in the settings file\n",
    "epochs = S['2Layer']['epochs']\n",
    "\n",
    "# The weights for cross-entropy specified in the setting file\n",
    "weights = S['2Layer']['weights']\n",
    "\n",
    "# The number of neurosn in the hiddne layers\n",
    "noNeurons = S['2Layer']['noNeurons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model\n",
    "model = Net2(noNeurons).to(DEVICE)\n",
    "M2 = fncs_helper.NetWrapper(model,DEVICE,epochs,weights)\n",
    "\n",
    "# Displaying a summary\n",
    "summary(M2.model,(1,INPUTSIZE))\n",
    "\n",
    "# Training the model\n",
    "M2.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Performance of predictions using the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "yTrainHat2 = M2.predict(xTrain)\n",
    "yValHat2 = M2.predict(xVal)\n",
    "\n",
    "print('RESULTS:\\n')\n",
    "fncs_helper.summaryPerf(yTrain,yTrainHat2,yVal,yValHat2)\n",
    "\n",
    "# Saving the predictions since they will be used for testing later\n",
    "np.savetxt('Data/2LayerPred.y.csv',yValHat2,fmt='%d')\n",
    "\n",
    "# Saving the model\n",
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save('Models/model_2Layer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Your task is to select the hyper-parameters specified in `Settings.json` for the 2-layer model so your performance exceeds 0.82 on balanced accuracy. Once you are done, can run the test below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "MLP2",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!python -m pytest fnc_2Layer_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### 3-Layer MLP\n",
    "\n",
    "When defining a 3-layer MLP, we have a choice for two of the neurons. We will set the first hidden layer to have the same number of layers as the optimal value found from the previous sections. We will also reuse the weights and number of epochs.\n",
    "\n",
    "In this case, you will have to complete the neural network below to have an additional hidden layer followed by a ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network architecture\n",
    "class Net3(nn.Module):\n",
    "  def __init__(self,noNeurons1,noNeurons2):\n",
    "    super(Net3, self).__init__()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        ### BEGIN SOLUTION\n",
    "        nn.Linear(INPUTSIZE,noNeurons1),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(noNeurons1,noNeurons2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(noNeurons2,OUTPUTSIZE),\n",
    "        ### END SOLUTION\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.linear_relu_stack(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "MLP3.1",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Reloading the settings in case you end up modifying them\n",
    "with open('Settings.json') as f:\n",
    "    S = json.load(f)\n",
    "    \n",
    "# Number of epochs specified in the settings file\n",
    "epochs = S['3Layer']['epochs']\n",
    "\n",
    "# The weights for cross-entropy specified in the setting file\n",
    "weights = S['3Layer']['weights']\n",
    "\n",
    "# The number of neurosn in the hiddne layers\n",
    "noNeurons1 = S['3Layer']['noNeurons1']\n",
    "noNeurons2 = S['3Layer']['noNeurons2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model\n",
    "model = Net3(noNeurons1,noNeurons2).to(DEVICE)\n",
    "M3 = fncs_helper.NetWrapper(model,DEVICE,epochs,weights)\n",
    "\n",
    "# Displaying a summary\n",
    "summary(M3.model,(1,INPUTSIZE))\n",
    "\n",
    "# Training the model\n",
    "M3.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Performance of predictions using the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainHat3 = M3.predict(xTrain)\n",
    "yValHat3 = M3.predict(xVal)\n",
    "\n",
    "print('RESULTS:\\n')\n",
    "fncs_helper.summaryPerf(yTrain,yTrainHat3,yVal,yValHat3)\n",
    "\n",
    "# Saving the predictions since they will be used for testing later\n",
    "np.savetxt('Data/3LayerPred.y.csv',yValHat3,fmt='%d')\n",
    "\n",
    "# Saving the model\n",
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save('Models/model_3Layer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Your task is to select the hyper-parameters specified in `Settings.json` for the 3-layer model so your performance exceeds 0.82 on balanced accuracy. Once you are done, can run the test below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "MLP3.2",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!python -m pytest fnc_3Layer_test.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
